{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3645,"status":"ok","timestamp":1680490802373,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"lPpM_wzKFIII","outputId":"72def817-b9be-4324-adeb-9e55520484e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1680490802374,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"BFzAy3OtF6US","outputId":"9ac6b9a8-fe39-4e89-8a50-8c941fe4c0e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680490802374,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"4fQtDzh0F8zE","outputId":"7c4e6a98-0f65-4fce-e74e-13c393eff8d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/assign03\n"]}],"source":["%cd /content/drive/MyDrive/Colab Notebooks/assign03"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680490802375,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"b4dZz_X9FGRV"},"outputs":[],"source":["import numpy as np \n","import matplotlib.pyplot as plt\n","\n","# install\n","## numpy\n","## matplotlib"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 손 글씨로 만들어진 0~9까지의 숫자 이미지 데이터셋"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Y6MaNVxpFGRX"},"source":["### data load & preprocessing"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680490802375,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"4g8-8OctFGRY","outputId":"2c999df0-ff84-421c-efe4-f7b1bdca908a"},"outputs":[{"name":"stdout","output_type":"stream","text":["(60000, 1, 28, 28)\n","(60000,)\n"]}],"source":["from dataset.mnist import load_mnist\n","\n","(train_raw_img, train_label), (test_raw_img, test_label) = load_mnist(flatten=False, normalize=False)\n","print(train_raw_img.shape)\n","print(train_label.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680490802375,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"uQ8U02CCFGRZ","outputId":"7a8cd15f-4903-471f-bd52-18e1869ce74f"},"outputs":[{"data":{"text/plain":["(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n"," array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]))"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["np.unique(train_label, return_counts=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1680490802376,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"xZnGA_YUFGRa"},"outputs":[],"source":["# train_dataset split according to the number\n","\n","new_train_img = [[] for _ in range(10)]\n","new_train_label = [[] for _ in range(10)]\n","\n","for i in range(len(train_label)) :\n","    new_train_img[train_label[i]].append(train_raw_img[i])\n","    new_train_label[train_label[i]].append(train_label[i])\n","\n","# print(len(new_train_img[0])) # 0에 해당하는 image 개수\n","# print(new_train_img[0][0].shape) # 0에 해당하는 image중 첫번째 image의 shape"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"z7tJ7XHTFGRb"},"source":["### 1. Create a classifier that distinguishes between zero and non-zero (using logistic regression)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### 데이터셋에서 주어진 이미지가 0인지 아닌지 구별하는 이진 분류를 하기 위한 Logistic Regression(로지스틱 회귀)를 작성하라"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1680490802376,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"MYkumpg9FGRb"},"outputs":[],"source":["# Target Number(idx)를 입력받아 Positive와 Negative Sample의 비율을 맞춰 학습 데이터셋 반환\n","def make_sample(idx) :\n","    sample_img = []\n","    sample_label = []\n","    \n","    # data sampling \n","    for i in range(10) :\n","        if i == idx :\n","            sample_img += new_train_img[i][:1000]\n","            sample_label += (new_train_label[i][:1000])\n","        else :\n","            sample_img += new_train_img[i][:111]\n","            sample_label += (new_train_label[i][:111])\n","\n","    sample_img = np.array(sample_img)\n","    sample_label = np.array(sample_label)\n","    \n","    # normalization (set value 0 ~ 1)\n","    sample_img = sample_img.astype('float')/255\n","    \n","    # target number는 1, 아니면 0\n","    sample_label = np.where(sample_label==idx, 1 ,0)\n","    \n","    # reshape\n","    sample_img = sample_img.reshape(len(sample_img.squeeze()), -1)\n","    sample_label = sample_label.reshape(len(sample_label.squeeze()), -1)\n","    \n","    return sample_img, sample_label"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":791,"status":"ok","timestamp":1680490803160,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"1IhWbvTIFGRb"},"outputs":[],"source":["train_X, train_y = make_sample(idx = 0) # idx = target number\n","train_X = np.insert(train_X, 0, 1, axis=1) # bias 추가\n","\n","# print(train_y.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680490803161,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"e8O2C8MLFGRc","outputId":"4c447926-4e3d-4f96-8526-04e3519e7bba"},"outputs":[{"name":"stdout","output_type":"stream","text":["loss :  8.063079315186753\n"]}],"source":["# cross entropy loss\n","def CrossEntropyLoss(preds, y) :\n","    delta = 1e-7\n","    loss = np.sum(-y*np.log(preds+delta)-(1-y)*np.log(1-preds+delta))/len(preds)\n","        \n","    return loss\n","\n","loss = CrossEntropyLoss(np.zeros((len(train_X), 1)), train_y)\n","print('loss : ', loss)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680490803161,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"qeVJQU2RFGRc"},"outputs":[],"source":["# Train 이미지와 Label을 입력받아 학습을 진행, 학습된 Parameter 반환\n","\n","def train(X, y) :\n","    \"\"\"_summary_\n","\n","    Args:\n","        X : train_X\n","        y : train_y\n","\n","    Returns:\n","        w : weight\n","    \"\"\"\n","    w = np.random.randn(len(X[0]), 1) # weight initialization\n","    lr = 0.1 # learning rate(수정 가능)\n","    step = 0\n","    acc = 0\n","    \n","    while (acc <= 0.9) :  # 좀 더 나은 정확성을 위해 acc이이 0.9 이상일 때 반복문 탈출\n","        step += 1\n","        \n","        # predict\n","        preds = 1 / (1 + np.exp(-np.dot(X, w)))\n","        loss = CrossEntropyLoss(preds, y)\n","        \n","        result = np.where(preds>0.5, 1, 0)\n","        acc = np.sum(np.where(result==y, True, False))/len(preds)\n","        \n","        print(\"total step : %d \" % step)\n","        print(\"error : %f, accuarcy : %f\" % (loss, acc))\n","        \n","        # gradient descent\n","        gradient = np.dot(X.T, (preds - y)) / len(preds)\n","        w -= lr * gradient\n","        \n","    return w"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680490803161,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"nWXi1kuJFGRc","outputId":"67f5406c-1d75-421f-f43c-14f205a0e403"},"outputs":[{"name":"stdout","output_type":"stream","text":["total step : 1 \n","error : 5.216721, accuarcy : 0.524762\n","total step : 2 \n","error : 4.768590, accuarcy : 0.535268\n","total step : 3 \n","error : 4.336914, accuarcy : 0.542271\n","total step : 4 \n","error : 3.941176, accuarcy : 0.544772\n","total step : 5 \n","error : 3.593916, accuarcy : 0.548274\n","total step : 6 \n","error : 3.301579, accuarcy : 0.567284\n","total step : 7 \n","error : 3.060888, accuarcy : 0.571786\n","total step : 8 \n","error : 2.862744, accuarcy : 0.579290\n","total step : 9 \n","error : 2.698728, accuarcy : 0.591796\n","total step : 10 \n","error : 2.561594, accuarcy : 0.603302\n","total step : 11 \n","error : 2.445090, accuarcy : 0.608804\n","total step : 12 \n","error : 2.344139, accuarcy : 0.613807\n","total step : 13 \n","error : 2.254881, accuarcy : 0.621811\n","total step : 14 \n","error : 2.174511, accuarcy : 0.624812\n","total step : 15 \n","error : 2.101057, accuarcy : 0.626313\n","total step : 16 \n","error : 2.033149, accuarcy : 0.634317\n","total step : 17 \n","error : 1.969815, accuarcy : 0.643322\n","total step : 18 \n","error : 1.910347, accuarcy : 0.651326\n","total step : 19 \n","error : 1.854206, accuarcy : 0.657829\n","total step : 20 \n","error : 1.800976, accuarcy : 0.666833\n","total step : 21 \n","error : 1.750331, accuarcy : 0.673337\n","total step : 22 \n","error : 1.702010, accuarcy : 0.679340\n","total step : 23 \n","error : 1.655808, accuarcy : 0.682841\n","total step : 24 \n","error : 1.611557, accuarcy : 0.684342\n","total step : 25 \n","error : 1.569124, accuarcy : 0.688344\n","total step : 26 \n","error : 1.528399, accuarcy : 0.693847\n","total step : 27 \n","error : 1.489289, accuarcy : 0.696348\n","total step : 28 \n","error : 1.451714, accuarcy : 0.703352\n","total step : 29 \n","error : 1.415603, accuarcy : 0.707854\n","total step : 30 \n","error : 1.380893, accuarcy : 0.712356\n","total step : 31 \n","error : 1.347524, accuarcy : 0.717359\n","total step : 32 \n","error : 1.315441, accuarcy : 0.723362\n","total step : 33 \n","error : 1.284591, accuarcy : 0.728364\n","total step : 34 \n","error : 1.254922, accuarcy : 0.731866\n","total step : 35 \n","error : 1.226387, accuarcy : 0.734867\n","total step : 36 \n","error : 1.198936, accuarcy : 0.738869\n","total step : 37 \n","error : 1.172524, accuarcy : 0.743872\n","total step : 38 \n","error : 1.147106, accuarcy : 0.748874\n","total step : 39 \n","error : 1.122637, accuarcy : 0.750375\n","total step : 40 \n","error : 1.099076, accuarcy : 0.755378\n","total step : 41 \n","error : 1.076383, accuarcy : 0.761381\n","total step : 42 \n","error : 1.054518, accuarcy : 0.763882\n","total step : 43 \n","error : 1.033443, accuarcy : 0.768384\n","total step : 44 \n","error : 1.013124, accuarcy : 0.772886\n","total step : 45 \n","error : 0.993525, accuarcy : 0.775888\n","total step : 46 \n","error : 0.974613, accuarcy : 0.780390\n","total step : 47 \n","error : 0.956357, accuarcy : 0.784892\n","total step : 48 \n","error : 0.938727, accuarcy : 0.786893\n","total step : 49 \n","error : 0.921694, accuarcy : 0.788394\n","total step : 50 \n","error : 0.905232, accuarcy : 0.790395\n","total step : 51 \n","error : 0.889314, accuarcy : 0.795898\n","total step : 52 \n","error : 0.873916, accuarcy : 0.797399\n","total step : 53 \n","error : 0.859016, accuarcy : 0.798399\n","total step : 54 \n","error : 0.844590, accuarcy : 0.800400\n","total step : 55 \n","error : 0.830619, accuarcy : 0.802901\n","total step : 56 \n","error : 0.817083, accuarcy : 0.805403\n","total step : 57 \n","error : 0.803964, accuarcy : 0.806903\n","total step : 58 \n","error : 0.791244, accuarcy : 0.809405\n","total step : 59 \n","error : 0.778906, accuarcy : 0.810905\n","total step : 60 \n","error : 0.766935, accuarcy : 0.813407\n","total step : 61 \n","error : 0.755316, accuarcy : 0.815908\n","total step : 62 \n","error : 0.744035, accuarcy : 0.816908\n","total step : 63 \n","error : 0.733078, accuarcy : 0.817909\n","total step : 64 \n","error : 0.722433, accuarcy : 0.820410\n","total step : 65 \n","error : 0.712086, accuarcy : 0.823412\n","total step : 66 \n","error : 0.702028, accuarcy : 0.824912\n","total step : 67 \n","error : 0.692246, accuarcy : 0.826913\n","total step : 68 \n","error : 0.682731, accuarcy : 0.829915\n","total step : 69 \n","error : 0.673472, accuarcy : 0.830415\n","total step : 70 \n","error : 0.664460, accuarcy : 0.832416\n","total step : 71 \n","error : 0.655686, accuarcy : 0.833917\n","total step : 72 \n","error : 0.647141, accuarcy : 0.836418\n","total step : 73 \n","error : 0.638818, accuarcy : 0.837919\n","total step : 74 \n","error : 0.630709, accuarcy : 0.838919\n","total step : 75 \n","error : 0.622806, accuarcy : 0.840420\n","total step : 76 \n","error : 0.615103, accuarcy : 0.843922\n","total step : 77 \n","error : 0.607593, accuarcy : 0.845423\n","total step : 78 \n","error : 0.600270, accuarcy : 0.846423\n","total step : 79 \n","error : 0.593127, accuarcy : 0.847924\n","total step : 80 \n","error : 0.586160, accuarcy : 0.848424\n","total step : 81 \n","error : 0.579361, accuarcy : 0.849425\n","total step : 82 \n","error : 0.572727, accuarcy : 0.849925\n","total step : 83 \n","error : 0.566253, accuarcy : 0.851926\n","total step : 84 \n","error : 0.559932, accuarcy : 0.852926\n","total step : 85 \n","error : 0.553761, accuarcy : 0.854427\n","total step : 86 \n","error : 0.547736, accuarcy : 0.856428\n","total step : 87 \n","error : 0.541851, accuarcy : 0.857429\n","total step : 88 \n","error : 0.536104, accuarcy : 0.859430\n","total step : 89 \n","error : 0.530488, accuarcy : 0.860930\n","total step : 90 \n","error : 0.525002, accuarcy : 0.861431\n","total step : 91 \n","error : 0.519641, accuarcy : 0.862431\n","total step : 92 \n","error : 0.514401, accuarcy : 0.862931\n","total step : 93 \n","error : 0.509279, accuarcy : 0.863932\n","total step : 94 \n","error : 0.504272, accuarcy : 0.865433\n","total step : 95 \n","error : 0.499376, accuarcy : 0.867934\n","total step : 96 \n","error : 0.494588, accuarcy : 0.868934\n","total step : 97 \n","error : 0.489904, accuarcy : 0.868434\n","total step : 98 \n","error : 0.485322, accuarcy : 0.869435\n","total step : 99 \n","error : 0.480840, accuarcy : 0.869435\n","total step : 100 \n","error : 0.476453, accuarcy : 0.871436\n","total step : 101 \n","error : 0.472159, accuarcy : 0.871436\n","total step : 102 \n","error : 0.467956, accuarcy : 0.871936\n","total step : 103 \n","error : 0.463841, accuarcy : 0.873937\n","total step : 104 \n","error : 0.459811, accuarcy : 0.874437\n","total step : 105 \n","error : 0.455864, accuarcy : 0.874937\n","total step : 106 \n","error : 0.451997, accuarcy : 0.875438\n","total step : 107 \n","error : 0.448209, accuarcy : 0.875938\n","total step : 108 \n","error : 0.444496, accuarcy : 0.876438\n","total step : 109 \n","error : 0.440858, accuarcy : 0.877939\n","total step : 110 \n","error : 0.437291, accuarcy : 0.878939\n","total step : 111 \n","error : 0.433793, accuarcy : 0.878939\n","total step : 112 \n","error : 0.430364, accuarcy : 0.879940\n","total step : 113 \n","error : 0.427000, accuarcy : 0.880440\n","total step : 114 \n","error : 0.423701, accuarcy : 0.881441\n","total step : 115 \n","error : 0.420463, accuarcy : 0.882441\n","total step : 116 \n","error : 0.417287, accuarcy : 0.882941\n","total step : 117 \n","error : 0.414169, accuarcy : 0.883442\n","total step : 118 \n","error : 0.411108, accuarcy : 0.883942\n","total step : 119 \n","error : 0.408103, accuarcy : 0.884442\n","total step : 120 \n","error : 0.405152, accuarcy : 0.885443\n","total step : 121 \n","error : 0.402255, accuarcy : 0.886943\n","total step : 122 \n","error : 0.399408, accuarcy : 0.887444\n","total step : 123 \n","error : 0.396612, accuarcy : 0.888444\n","total step : 124 \n","error : 0.393864, accuarcy : 0.888944\n","total step : 125 \n","error : 0.391164, accuarcy : 0.891946\n","total step : 126 \n","error : 0.388511, accuarcy : 0.893447\n","total step : 127 \n","error : 0.385902, accuarcy : 0.893447\n","total step : 128 \n","error : 0.383338, accuarcy : 0.893447\n","total step : 129 \n","error : 0.380816, accuarcy : 0.893947\n","total step : 130 \n","error : 0.378336, accuarcy : 0.894947\n","total step : 131 \n","error : 0.375897, accuarcy : 0.895948\n","total step : 132 \n","error : 0.373498, accuarcy : 0.898449\n","total step : 133 \n","error : 0.371137, accuarcy : 0.899950\n","total step : 134 \n","error : 0.368815, accuarcy : 0.900950\n"]}],"source":["# save weight\n","\n","test_w = train(train_X, train_y)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1680490803162,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"SLhEZ1GyFGRd","outputId":"4a6a1943-159c-4c04-e0fa-c5390feccf91"},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy :  0.8924\n"]}],"source":["# Target Number(idx)와 학습된 Parameter를 입력받아 Accuracy 계산\n","\n","def eval(idx, w) :\n","    \"\"\"_summary_\n","\n","    Args:\n","        idx : target_number\n","        w : parameter\n","    \"\"\"\n","    test_X = test_raw_img.astype('float')/255    \n","    test_X = test_X.reshape(len(test_X.squeeze()), -1)\n","    test_X = np.insert(test_X, 0, 1, axis=1) # + bias\n","\n","    test_y = np.where(test_label==idx, 1 ,0)\n","    test_y = test_y.reshape(len(test_y.squeeze()), -1)\n","    \n","    preds = 1/(1+np.exp(-test_X.dot(w)))\n","    result = np.where(preds>0.5, 1, 0)\n","    \n","    acc = np.sum(np.where(result==test_y, True, False))/len(preds)\n","    print('accuracy : ', acc)\n","\n","eval(idx=0, w=test_w)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"itKbnbmNFGRd"},"source":["### 2. multi class single label classification (using logistic regression)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### 데이터셋을 Logistic Regression(로지스틱 회귀)를 이용하여 N개의 멀티 분류 모델을 활용해 주어진 이미지의 숫자를 예측"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5623,"status":"ok","timestamp":1680490808781,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"ExZROSKHFGRd"},"outputs":[],"source":["# train several binary linear regression classifier (0~9)\n","classifiers = []\n","for i in range(10) :\n","  train_X, train_y = make_sample(i) # i = target number\n","  train_X = np.insert(train_X, 0, 1, axis=1) # bias 추가\n","  clf = LogisticRegression(max_iter=5000) # Logistic Regression 이용용\n","  clf.fit(train_X, train_y.ravel())\n","  classifiers.append(clf)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":693,"status":"ok","timestamp":1680490809459,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"mOkzi2x5FGRd","outputId":"2a256943-6d61-41ed-ae56-210be7c9a2f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["shape of test_X :  (10000, 785)\n","shape of test_label :  (10000,)\n","accuracy :  0.8824\n"]}],"source":["# eval\n","\n","test_X = test_raw_img.astype('float')/255    \n","test_X = test_X.reshape(len(test_X.squeeze()), -1)\n","test_X = np.insert(test_X, 0, 1, axis=1) # + bias\n","print('shape of test_X : ', test_X.shape)\n","print('shape of test_label : ', test_label.shape)\n","\n","probabilities = []\n","for clf in classifiers:\n","  proba = clf.predict_proba(test_X)[:, 1]\n","  probabilities.append(proba)\n","\n","probabilities = np.array(probabilities).T\n","\n","# make prediction using argmax\n","max_pred = np.argmax(probabilities, axis=1) # shape : (10000,1) or (10000,)\n","\n","acc = np.sum(np.where(test_label==max_pred, True, False))/len(test_X)\n","print('accuracy : ', acc)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"awUM0AQKv67R"},"source":["### L2 Regularization"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Overfitting 방지를 위한 L2 Regularization 구현"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3397,"status":"ok","timestamp":1680490896834,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"OnkUy_Swv67S","outputId":"79b36c64-d639-4cdf-d6a4-816441973fad"},"outputs":[{"name":"stdout","output_type":"stream","text":["total step : 1 \n","error : 6.467850, accuarcy : 0.491246\n","total step : 2 \n","error : 5.836976, accuarcy : 0.487744\n","total step : 3 \n","error : 5.097173, accuarcy : 0.490745\n","total step : 4 \n","error : 4.299391, accuarcy : 0.493747\n","total step : 5 \n","error : 3.529770, accuarcy : 0.493747\n","total step : 6 \n","error : 2.879032, accuarcy : 0.511756\n","total step : 7 \n","error : 2.401287, accuarcy : 0.536768\n","total step : 8 \n","error : 2.089865, accuarcy : 0.562781\n","total step : 9 \n","error : 1.897851, accuarcy : 0.579290\n","total step : 10 \n","error : 1.776583, accuarcy : 0.597799\n","total step : 11 \n","error : 1.694471, accuarcy : 0.606803\n","total step : 12 \n","error : 1.634014, accuarcy : 0.617809\n","total step : 13 \n","error : 1.585846, accuarcy : 0.628814\n","total step : 14 \n","error : 1.544969, accuarcy : 0.637319\n","total step : 15 \n","error : 1.508699, accuarcy : 0.642321\n","total step : 16 \n","error : 1.475578, accuarcy : 0.647324\n","total step : 17 \n","error : 1.444791, accuarcy : 0.654327\n","total step : 18 \n","error : 1.415870, accuarcy : 0.659330\n","total step : 19 \n","error : 1.388530, accuarcy : 0.662831\n","total step : 20 \n","error : 1.362585, accuarcy : 0.665833\n","total step : 21 \n","error : 1.337908, accuarcy : 0.668834\n","total step : 22 \n","error : 1.314401, accuarcy : 0.670835\n","total step : 23 \n","error : 1.291986, accuarcy : 0.677339\n","total step : 24 \n","error : 1.270600, accuarcy : 0.683342\n","total step : 25 \n","error : 1.250183, accuarcy : 0.687844\n","total step : 26 \n","error : 1.230685, accuarcy : 0.692846\n","total step : 27 \n","error : 1.212057, accuarcy : 0.695348\n","total step : 28 \n","error : 1.194254, accuarcy : 0.699850\n","total step : 29 \n","error : 1.177233, accuarcy : 0.701851\n","total step : 30 \n","error : 1.160955, accuarcy : 0.704352\n","total step : 31 \n","error : 1.145380, accuarcy : 0.708354\n","total step : 32 \n","error : 1.130471, accuarcy : 0.711856\n","total step : 33 \n","error : 1.116194, accuarcy : 0.713357\n","total step : 34 \n","error : 1.102514, accuarcy : 0.716358\n","total step : 35 \n","error : 1.089400, accuarcy : 0.720360\n","total step : 36 \n","error : 1.076821, accuarcy : 0.721361\n","total step : 37 \n","error : 1.064748, accuarcy : 0.723362\n","total step : 38 \n","error : 1.053154, accuarcy : 0.726863\n","total step : 39 \n","error : 1.042016, accuarcy : 0.729865\n","total step : 40 \n","error : 1.031308, accuarcy : 0.733367\n","total step : 41 \n","error : 1.021010, accuarcy : 0.735868\n","total step : 42 \n","error : 1.011101, accuarcy : 0.736868\n","total step : 43 \n","error : 1.001562, accuarcy : 0.739370\n","total step : 44 \n","error : 0.992374, accuarcy : 0.741371\n","total step : 45 \n","error : 0.983522, accuarcy : 0.744872\n","total step : 46 \n","error : 0.974989, accuarcy : 0.746373\n","total step : 47 \n","error : 0.966759, accuarcy : 0.748374\n","total step : 48 \n","error : 0.958818, accuarcy : 0.749875\n","total step : 49 \n","error : 0.951153, accuarcy : 0.752376\n","total step : 50 \n","error : 0.943749, accuarcy : 0.752876\n","total step : 51 \n","error : 0.936593, accuarcy : 0.754877\n","total step : 52 \n","error : 0.929673, accuarcy : 0.758879\n","total step : 53 \n","error : 0.922978, accuarcy : 0.759380\n","total step : 54 \n","error : 0.916494, accuarcy : 0.761881\n","total step : 55 \n","error : 0.910212, accuarcy : 0.761881\n","total step : 56 \n","error : 0.904120, accuarcy : 0.763882\n","total step : 57 \n","error : 0.898208, accuarcy : 0.765383\n","total step : 58 \n","error : 0.892468, accuarcy : 0.766883\n","total step : 59 \n","error : 0.886889, accuarcy : 0.768384\n","total step : 60 \n","error : 0.881463, accuarcy : 0.768384\n","total step : 61 \n","error : 0.876183, accuarcy : 0.769885\n","total step : 62 \n","error : 0.871040, accuarcy : 0.770885\n","total step : 63 \n","error : 0.866028, accuarcy : 0.770385\n","total step : 64 \n","error : 0.861139, accuarcy : 0.771886\n","total step : 65 \n","error : 0.856368, accuarcy : 0.773387\n","total step : 66 \n","error : 0.851709, accuarcy : 0.775388\n","total step : 67 \n","error : 0.847157, accuarcy : 0.777389\n","total step : 68 \n","error : 0.842705, accuarcy : 0.778889\n","total step : 69 \n","error : 0.838351, accuarcy : 0.779390\n","total step : 70 \n","error : 0.834088, accuarcy : 0.779890\n","total step : 71 \n","error : 0.829913, accuarcy : 0.781391\n","total step : 72 \n","error : 0.825823, accuarcy : 0.782391\n","total step : 73 \n","error : 0.821813, accuarcy : 0.783892\n","total step : 74 \n","error : 0.817880, accuarcy : 0.784392\n","total step : 75 \n","error : 0.814021, accuarcy : 0.785393\n","total step : 76 \n","error : 0.810233, accuarcy : 0.785893\n","total step : 77 \n","error : 0.806513, accuarcy : 0.787394\n","total step : 78 \n","error : 0.802858, accuarcy : 0.787394\n","total step : 79 \n","error : 0.799267, accuarcy : 0.788894\n","total step : 80 \n","error : 0.795736, accuarcy : 0.789895\n","total step : 81 \n","error : 0.792264, accuarcy : 0.790395\n","total step : 82 \n","error : 0.788849, accuarcy : 0.790895\n","total step : 83 \n","error : 0.785488, accuarcy : 0.791896\n","total step : 84 \n","error : 0.782179, accuarcy : 0.792896\n","total step : 85 \n","error : 0.778922, accuarcy : 0.793397\n","total step : 86 \n","error : 0.775714, accuarcy : 0.795398\n","total step : 87 \n","error : 0.772553, accuarcy : 0.795898\n","total step : 88 \n","error : 0.769439, accuarcy : 0.796398\n","total step : 89 \n","error : 0.766370, accuarcy : 0.798899\n","total step : 90 \n","error : 0.763344, accuarcy : 0.799400\n","total step : 91 \n","error : 0.760360, accuarcy : 0.799400\n","total step : 92 \n","error : 0.757417, accuarcy : 0.800400\n","total step : 93 \n","error : 0.754514, accuarcy : 0.800400\n","total step : 94 \n","error : 0.751650, accuarcy : 0.801401\n","total step : 95 \n","error : 0.748823, accuarcy : 0.801901\n","total step : 96 \n","error : 0.746033, accuarcy : 0.802401\n","total step : 97 \n","error : 0.743278, accuarcy : 0.803902\n","total step : 98 \n","error : 0.740558, accuarcy : 0.803902\n","total step : 99 \n","error : 0.737872, accuarcy : 0.804902\n","total step : 100 \n","error : 0.735218, accuarcy : 0.804402\n","total step : 101 \n","error : 0.732597, accuarcy : 0.804402\n","total step : 102 \n","error : 0.730007, accuarcy : 0.804402\n","total step : 103 \n","error : 0.727447, accuarcy : 0.804902\n","total step : 104 \n","error : 0.724917, accuarcy : 0.805903\n","total step : 105 \n","error : 0.722416, accuarcy : 0.805903\n","total step : 106 \n","error : 0.719944, accuarcy : 0.807904\n","total step : 107 \n","error : 0.717499, accuarcy : 0.808404\n","total step : 108 \n","error : 0.715081, accuarcy : 0.808904\n","total step : 109 \n","error : 0.712690, accuarcy : 0.808904\n","total step : 110 \n","error : 0.710324, accuarcy : 0.809405\n","total step : 111 \n","error : 0.707984, accuarcy : 0.809905\n","total step : 112 \n","error : 0.705669, accuarcy : 0.809905\n","total step : 113 \n","error : 0.703378, accuarcy : 0.811406\n","total step : 114 \n","error : 0.701110, accuarcy : 0.811906\n","total step : 115 \n","error : 0.698866, accuarcy : 0.812406\n","total step : 116 \n","error : 0.696644, accuarcy : 0.813407\n","total step : 117 \n","error : 0.694445, accuarcy : 0.813907\n","total step : 118 \n","error : 0.692267, accuarcy : 0.813907\n","total step : 119 \n","error : 0.690111, accuarcy : 0.813907\n","total step : 120 \n","error : 0.687975, accuarcy : 0.814907\n","total step : 121 \n","error : 0.685861, accuarcy : 0.815408\n","total step : 122 \n","error : 0.683766, accuarcy : 0.815908\n","total step : 123 \n","error : 0.681691, accuarcy : 0.815908\n","total step : 124 \n","error : 0.679635, accuarcy : 0.817409\n","total step : 125 \n","error : 0.677599, accuarcy : 0.817409\n","total step : 126 \n","error : 0.675581, accuarcy : 0.816908\n","total step : 127 \n","error : 0.673581, accuarcy : 0.817409\n","total step : 128 \n","error : 0.671600, accuarcy : 0.818409\n","total step : 129 \n","error : 0.669636, accuarcy : 0.819910\n","total step : 130 \n","error : 0.667690, accuarcy : 0.820410\n","total step : 131 \n","error : 0.665760, accuarcy : 0.820910\n","total step : 132 \n","error : 0.663847, accuarcy : 0.820910\n","total step : 133 \n","error : 0.661951, accuarcy : 0.821911\n","total step : 134 \n","error : 0.660071, accuarcy : 0.822911\n","total step : 135 \n","error : 0.658207, accuarcy : 0.823412\n","total step : 136 \n","error : 0.656359, accuarcy : 0.823912\n","total step : 137 \n","error : 0.654526, accuarcy : 0.823912\n","total step : 138 \n","error : 0.652708, accuarcy : 0.823912\n","total step : 139 \n","error : 0.650906, accuarcy : 0.823912\n","total step : 140 \n","error : 0.649118, accuarcy : 0.824912\n","total step : 141 \n","error : 0.647344, accuarcy : 0.825413\n","total step : 142 \n","error : 0.645585, accuarcy : 0.825913\n","total step : 143 \n","error : 0.643840, accuarcy : 0.827414\n","total step : 144 \n","error : 0.642108, accuarcy : 0.827414\n","total step : 145 \n","error : 0.640391, accuarcy : 0.827414\n","total step : 146 \n","error : 0.638686, accuarcy : 0.827914\n","total step : 147 \n","error : 0.636995, accuarcy : 0.828414\n","total step : 148 \n","error : 0.635317, accuarcy : 0.828914\n","total step : 149 \n","error : 0.633652, accuarcy : 0.829915\n","total step : 150 \n","error : 0.632000, accuarcy : 0.829915\n","total step : 151 \n","error : 0.630360, accuarcy : 0.829915\n","total step : 152 \n","error : 0.628732, accuarcy : 0.830415\n","total step : 153 \n","error : 0.627117, accuarcy : 0.830415\n","total step : 154 \n","error : 0.625514, accuarcy : 0.830915\n","total step : 155 \n","error : 0.623922, accuarcy : 0.830915\n","total step : 156 \n","error : 0.622342, accuarcy : 0.830915\n","total step : 157 \n","error : 0.620774, accuarcy : 0.831416\n","total step : 158 \n","error : 0.619217, accuarcy : 0.831416\n","total step : 159 \n","error : 0.617672, accuarcy : 0.831416\n","total step : 160 \n","error : 0.616138, accuarcy : 0.832416\n","total step : 161 \n","error : 0.614614, accuarcy : 0.832416\n","total step : 162 \n","error : 0.613101, accuarcy : 0.832916\n","total step : 163 \n","error : 0.611600, accuarcy : 0.832916\n","total step : 164 \n","error : 0.610108, accuarcy : 0.833417\n","total step : 165 \n","error : 0.608627, accuarcy : 0.833417\n","total step : 166 \n","error : 0.607157, accuarcy : 0.833917\n","total step : 167 \n","error : 0.605697, accuarcy : 0.834917\n","total step : 168 \n","error : 0.604247, accuarcy : 0.834917\n","total step : 169 \n","error : 0.602806, accuarcy : 0.835418\n","total step : 170 \n","error : 0.601376, accuarcy : 0.835418\n","total step : 171 \n","error : 0.599955, accuarcy : 0.835918\n","total step : 172 \n","error : 0.598544, accuarcy : 0.837419\n","total step : 173 \n","error : 0.597143, accuarcy : 0.837919\n","total step : 174 \n","error : 0.595751, accuarcy : 0.838419\n","total step : 175 \n","error : 0.594368, accuarcy : 0.838919\n","total step : 176 \n","error : 0.592995, accuarcy : 0.839420\n","total step : 177 \n","error : 0.591631, accuarcy : 0.839420\n","total step : 178 \n","error : 0.590275, accuarcy : 0.839920\n","total step : 179 \n","error : 0.588929, accuarcy : 0.840920\n","total step : 180 \n","error : 0.587591, accuarcy : 0.841421\n","total step : 181 \n","error : 0.586263, accuarcy : 0.842421\n","total step : 182 \n","error : 0.584942, accuarcy : 0.842421\n","total step : 183 \n","error : 0.583631, accuarcy : 0.842921\n","total step : 184 \n","error : 0.582328, accuarcy : 0.842921\n","total step : 185 \n","error : 0.581033, accuarcy : 0.842921\n","total step : 186 \n","error : 0.579747, accuarcy : 0.842921\n","total step : 187 \n","error : 0.578468, accuarcy : 0.843422\n","total step : 188 \n","error : 0.577198, accuarcy : 0.843422\n","total step : 189 \n","error : 0.575936, accuarcy : 0.843922\n","total step : 190 \n","error : 0.574682, accuarcy : 0.843922\n","total step : 191 \n","error : 0.573436, accuarcy : 0.844922\n","total step : 192 \n","error : 0.572198, accuarcy : 0.844922\n","total step : 193 \n","error : 0.570967, accuarcy : 0.845423\n","total step : 194 \n","error : 0.569744, accuarcy : 0.845923\n","total step : 195 \n","error : 0.568529, accuarcy : 0.845923\n","total step : 196 \n","error : 0.567321, accuarcy : 0.846423\n","total step : 197 \n","error : 0.566120, accuarcy : 0.847424\n","total step : 198 \n","error : 0.564927, accuarcy : 0.847424\n","total step : 199 \n","error : 0.563742, accuarcy : 0.847424\n","total step : 200 \n","error : 0.562563, accuarcy : 0.847424\n","total step : 201 \n","error : 0.561392, accuarcy : 0.847424\n","total step : 202 \n","error : 0.560228, accuarcy : 0.847424\n","total step : 203 \n","error : 0.559070, accuarcy : 0.847924\n","total step : 204 \n","error : 0.557920, accuarcy : 0.848424\n","total step : 205 \n","error : 0.556777, accuarcy : 0.848924\n","total step : 206 \n","error : 0.555640, accuarcy : 0.849925\n","total step : 207 \n","error : 0.554511, accuarcy : 0.849925\n","total step : 208 \n","error : 0.553387, accuarcy : 0.850425\n","total step : 209 \n","error : 0.552271, accuarcy : 0.850925\n","total step : 210 \n","error : 0.551161, accuarcy : 0.850925\n","total step : 211 \n","error : 0.550058, accuarcy : 0.850925\n","total step : 212 \n","error : 0.548961, accuarcy : 0.851426\n","total step : 213 \n","error : 0.547871, accuarcy : 0.851426\n","total step : 214 \n","error : 0.546787, accuarcy : 0.851426\n","total step : 215 \n","error : 0.545709, accuarcy : 0.851426\n","total step : 216 \n","error : 0.544638, accuarcy : 0.851426\n","total step : 217 \n","error : 0.543572, accuarcy : 0.851426\n","total step : 218 \n","error : 0.542513, accuarcy : 0.851426\n","total step : 219 \n","error : 0.541460, accuarcy : 0.852426\n","total step : 220 \n","error : 0.540413, accuarcy : 0.852426\n","total step : 221 \n","error : 0.539372, accuarcy : 0.852926\n","total step : 222 \n","error : 0.538337, accuarcy : 0.852926\n","total step : 223 \n","error : 0.537307, accuarcy : 0.852926\n","total step : 224 \n","error : 0.536284, accuarcy : 0.852926\n","total step : 225 \n","error : 0.535266, accuarcy : 0.852926\n","total step : 226 \n","error : 0.534254, accuarcy : 0.852926\n","total step : 227 \n","error : 0.533248, accuarcy : 0.852926\n","total step : 228 \n","error : 0.532247, accuarcy : 0.852926\n","total step : 229 \n","error : 0.531252, accuarcy : 0.852926\n","total step : 230 \n","error : 0.530262, accuarcy : 0.853427\n","total step : 231 \n","error : 0.529278, accuarcy : 0.853927\n","total step : 232 \n","error : 0.528299, accuarcy : 0.854427\n","total step : 233 \n","error : 0.527326, accuarcy : 0.854927\n","total step : 234 \n","error : 0.526358, accuarcy : 0.854927\n","total step : 235 \n","error : 0.525395, accuarcy : 0.855928\n","total step : 236 \n","error : 0.524437, accuarcy : 0.856928\n","total step : 237 \n","error : 0.523485, accuarcy : 0.856928\n","total step : 238 \n","error : 0.522538, accuarcy : 0.856928\n","total step : 239 \n","error : 0.521596, accuarcy : 0.856928\n","total step : 240 \n","error : 0.520659, accuarcy : 0.856928\n","total step : 241 \n","error : 0.519727, accuarcy : 0.858429\n","total step : 242 \n","error : 0.518800, accuarcy : 0.858429\n","total step : 243 \n","error : 0.517877, accuarcy : 0.859930\n","total step : 244 \n","error : 0.516960, accuarcy : 0.859930\n","total step : 245 \n","error : 0.516048, accuarcy : 0.860430\n","total step : 246 \n","error : 0.515140, accuarcy : 0.860430\n","total step : 247 \n","error : 0.514238, accuarcy : 0.860430\n","total step : 248 \n","error : 0.513340, accuarcy : 0.860430\n","total step : 249 \n","error : 0.512446, accuarcy : 0.860930\n","total step : 250 \n","error : 0.511557, accuarcy : 0.860930\n","total step : 251 \n","error : 0.510673, accuarcy : 0.860930\n","total step : 252 \n","error : 0.509794, accuarcy : 0.860930\n","total step : 253 \n","error : 0.508919, accuarcy : 0.860930\n","total step : 254 \n","error : 0.508048, accuarcy : 0.860930\n","total step : 255 \n","error : 0.507182, accuarcy : 0.860930\n","total step : 256 \n","error : 0.506321, accuarcy : 0.861431\n","total step : 257 \n","error : 0.505464, accuarcy : 0.861431\n","total step : 258 \n","error : 0.504611, accuarcy : 0.861931\n","total step : 259 \n","error : 0.503762, accuarcy : 0.861931\n","total step : 260 \n","error : 0.502918, accuarcy : 0.861931\n","total step : 261 \n","error : 0.502078, accuarcy : 0.861931\n","total step : 262 \n","error : 0.501242, accuarcy : 0.862431\n","total step : 263 \n","error : 0.500411, accuarcy : 0.863432\n","total step : 264 \n","error : 0.499583, accuarcy : 0.863432\n","total step : 265 \n","error : 0.498760, accuarcy : 0.863932\n","total step : 266 \n","error : 0.497941, accuarcy : 0.863932\n","total step : 267 \n","error : 0.497126, accuarcy : 0.864432\n","total step : 268 \n","error : 0.496315, accuarcy : 0.865433\n","total step : 269 \n","error : 0.495507, accuarcy : 0.865433\n","total step : 270 \n","error : 0.494704, accuarcy : 0.865433\n","total step : 271 \n","error : 0.493905, accuarcy : 0.865433\n","total step : 272 \n","error : 0.493109, accuarcy : 0.865933\n","total step : 273 \n","error : 0.492318, accuarcy : 0.865933\n","total step : 274 \n","error : 0.491530, accuarcy : 0.865933\n","total step : 275 \n","error : 0.490746, accuarcy : 0.865933\n","total step : 276 \n","error : 0.489966, accuarcy : 0.865933\n","total step : 277 \n","error : 0.489190, accuarcy : 0.865933\n","total step : 278 \n","error : 0.488417, accuarcy : 0.865933\n","total step : 279 \n","error : 0.487648, accuarcy : 0.865933\n","total step : 280 \n","error : 0.486882, accuarcy : 0.865933\n","total step : 281 \n","error : 0.486121, accuarcy : 0.865933\n","total step : 282 \n","error : 0.485363, accuarcy : 0.865933\n","total step : 283 \n","error : 0.484608, accuarcy : 0.866433\n","total step : 284 \n","error : 0.483857, accuarcy : 0.865933\n","total step : 285 \n","error : 0.483109, accuarcy : 0.865933\n","total step : 286 \n","error : 0.482365, accuarcy : 0.865933\n","total step : 287 \n","error : 0.481625, accuarcy : 0.865933\n","total step : 288 \n","error : 0.480888, accuarcy : 0.865933\n","total step : 289 \n","error : 0.480154, accuarcy : 0.865933\n","total step : 290 \n","error : 0.479423, accuarcy : 0.865933\n","total step : 291 \n","error : 0.478696, accuarcy : 0.865933\n","total step : 292 \n","error : 0.477973, accuarcy : 0.865933\n","total step : 293 \n","error : 0.477252, accuarcy : 0.865933\n","total step : 294 \n","error : 0.476535, accuarcy : 0.865933\n","total step : 295 \n","error : 0.475821, accuarcy : 0.866433\n","total step : 296 \n","error : 0.475111, accuarcy : 0.866433\n","total step : 297 \n","error : 0.474403, accuarcy : 0.866433\n","total step : 298 \n","error : 0.473699, accuarcy : 0.866433\n","total step : 299 \n","error : 0.472998, accuarcy : 0.866433\n","total step : 300 \n","error : 0.472300, accuarcy : 0.866933\n","total step : 301 \n","error : 0.471605, accuarcy : 0.866933\n","total step : 302 \n","error : 0.470914, accuarcy : 0.866933\n","total step : 303 \n","error : 0.470225, accuarcy : 0.866933\n","total step : 304 \n","error : 0.469539, accuarcy : 0.867434\n","total step : 305 \n","error : 0.468857, accuarcy : 0.867434\n","total step : 306 \n","error : 0.468177, accuarcy : 0.868434\n","total step : 307 \n","error : 0.467500, accuarcy : 0.868434\n","total step : 308 \n","error : 0.466827, accuarcy : 0.868934\n","total step : 309 \n","error : 0.466156, accuarcy : 0.868934\n","total step : 310 \n","error : 0.465488, accuarcy : 0.868934\n","total step : 311 \n","error : 0.464823, accuarcy : 0.868934\n","total step : 312 \n","error : 0.464161, accuarcy : 0.869435\n","total step : 313 \n","error : 0.463502, accuarcy : 0.869435\n","total step : 314 \n","error : 0.462846, accuarcy : 0.869935\n","total step : 315 \n","error : 0.462192, accuarcy : 0.869935\n","total step : 316 \n","error : 0.461542, accuarcy : 0.869935\n","total step : 317 \n","error : 0.460894, accuarcy : 0.869935\n","total step : 318 \n","error : 0.460248, accuarcy : 0.869935\n","total step : 319 \n","error : 0.459606, accuarcy : 0.869935\n","total step : 320 \n","error : 0.458966, accuarcy : 0.869935\n","total step : 321 \n","error : 0.458329, accuarcy : 0.869935\n","total step : 322 \n","error : 0.457695, accuarcy : 0.869935\n","total step : 323 \n","error : 0.457063, accuarcy : 0.869935\n","total step : 324 \n","error : 0.456434, accuarcy : 0.869935\n","total step : 325 \n","error : 0.455807, accuarcy : 0.869435\n","total step : 326 \n","error : 0.455184, accuarcy : 0.869935\n","total step : 327 \n","error : 0.454562, accuarcy : 0.869935\n","total step : 328 \n","error : 0.453944, accuarcy : 0.869935\n","total step : 329 \n","error : 0.453327, accuarcy : 0.869935\n","total step : 330 \n","error : 0.452714, accuarcy : 0.869935\n","total step : 331 \n","error : 0.452103, accuarcy : 0.869935\n","total step : 332 \n","error : 0.451494, accuarcy : 0.870435\n","total step : 333 \n","error : 0.450888, accuarcy : 0.870435\n","total step : 334 \n","error : 0.450284, accuarcy : 0.870935\n","total step : 335 \n","error : 0.449683, accuarcy : 0.870935\n","total step : 336 \n","error : 0.449085, accuarcy : 0.870935\n","total step : 337 \n","error : 0.448488, accuarcy : 0.870935\n","total step : 338 \n","error : 0.447894, accuarcy : 0.870935\n","total step : 339 \n","error : 0.447303, accuarcy : 0.870935\n","total step : 340 \n","error : 0.446714, accuarcy : 0.870935\n","total step : 341 \n","error : 0.446127, accuarcy : 0.871436\n","total step : 342 \n","error : 0.445542, accuarcy : 0.871436\n","total step : 343 \n","error : 0.444960, accuarcy : 0.871436\n","total step : 344 \n","error : 0.444380, accuarcy : 0.871436\n","total step : 345 \n","error : 0.443803, accuarcy : 0.871936\n","total step : 346 \n","error : 0.443227, accuarcy : 0.871936\n","total step : 347 \n","error : 0.442654, accuarcy : 0.871936\n","total step : 348 \n","error : 0.442084, accuarcy : 0.872436\n","total step : 349 \n","error : 0.441515, accuarcy : 0.872436\n","total step : 350 \n","error : 0.440949, accuarcy : 0.872436\n","total step : 351 \n","error : 0.440385, accuarcy : 0.872436\n","total step : 352 \n","error : 0.439823, accuarcy : 0.872936\n","total step : 353 \n","error : 0.439263, accuarcy : 0.873437\n","total step : 354 \n","error : 0.438705, accuarcy : 0.873437\n","total step : 355 \n","error : 0.438150, accuarcy : 0.873437\n","total step : 356 \n","error : 0.437597, accuarcy : 0.873437\n","total step : 357 \n","error : 0.437046, accuarcy : 0.873437\n","total step : 358 \n","error : 0.436497, accuarcy : 0.873437\n","total step : 359 \n","error : 0.435950, accuarcy : 0.873437\n","total step : 360 \n","error : 0.435405, accuarcy : 0.873437\n","total step : 361 \n","error : 0.434862, accuarcy : 0.872936\n","total step : 362 \n","error : 0.434321, accuarcy : 0.872936\n","total step : 363 \n","error : 0.433783, accuarcy : 0.872936\n","total step : 364 \n","error : 0.433246, accuarcy : 0.872936\n","total step : 365 \n","error : 0.432711, accuarcy : 0.872936\n","total step : 366 \n","error : 0.432179, accuarcy : 0.872936\n","total step : 367 \n","error : 0.431648, accuarcy : 0.873937\n","total step : 368 \n","error : 0.431119, accuarcy : 0.873937\n","total step : 369 \n","error : 0.430593, accuarcy : 0.873937\n","total step : 370 \n","error : 0.430068, accuarcy : 0.874437\n","total step : 371 \n","error : 0.429545, accuarcy : 0.874937\n","total step : 372 \n","error : 0.429024, accuarcy : 0.874937\n","total step : 373 \n","error : 0.428505, accuarcy : 0.874937\n","total step : 374 \n","error : 0.427988, accuarcy : 0.874937\n","total step : 375 \n","error : 0.427473, accuarcy : 0.874937\n","total step : 376 \n","error : 0.426960, accuarcy : 0.875938\n","total step : 377 \n","error : 0.426449, accuarcy : 0.875938\n","total step : 378 \n","error : 0.425939, accuarcy : 0.875938\n","total step : 379 \n","error : 0.425431, accuarcy : 0.876438\n","total step : 380 \n","error : 0.424926, accuarcy : 0.876438\n","total step : 381 \n","error : 0.424422, accuarcy : 0.875938\n","total step : 382 \n","error : 0.423920, accuarcy : 0.875938\n","total step : 383 \n","error : 0.423419, accuarcy : 0.875938\n","total step : 384 \n","error : 0.422921, accuarcy : 0.876438\n","total step : 385 \n","error : 0.422424, accuarcy : 0.876438\n","total step : 386 \n","error : 0.421929, accuarcy : 0.876438\n","total step : 387 \n","error : 0.421436, accuarcy : 0.877439\n","total step : 388 \n","error : 0.420944, accuarcy : 0.877439\n","total step : 389 \n","error : 0.420455, accuarcy : 0.877939\n","total step : 390 \n","error : 0.419967, accuarcy : 0.878439\n","total step : 391 \n","error : 0.419481, accuarcy : 0.878439\n","total step : 392 \n","error : 0.418996, accuarcy : 0.878939\n","total step : 393 \n","error : 0.418513, accuarcy : 0.878939\n","total step : 394 \n","error : 0.418032, accuarcy : 0.878939\n","total step : 395 \n","error : 0.417553, accuarcy : 0.878939\n","total step : 396 \n","error : 0.417075, accuarcy : 0.878939\n","total step : 397 \n","error : 0.416599, accuarcy : 0.878939\n","total step : 398 \n","error : 0.416125, accuarcy : 0.879440\n","total step : 399 \n","error : 0.415652, accuarcy : 0.879440\n","total step : 400 \n","error : 0.415181, accuarcy : 0.879940\n","total step : 401 \n","error : 0.414711, accuarcy : 0.880440\n","total step : 402 \n","error : 0.414243, accuarcy : 0.880440\n","total step : 403 \n","error : 0.413777, accuarcy : 0.880440\n","total step : 404 \n","error : 0.413312, accuarcy : 0.880440\n","total step : 405 \n","error : 0.412849, accuarcy : 0.880440\n","total step : 406 \n","error : 0.412388, accuarcy : 0.880440\n","total step : 407 \n","error : 0.411928, accuarcy : 0.880440\n","total step : 408 \n","error : 0.411470, accuarcy : 0.880440\n","total step : 409 \n","error : 0.411013, accuarcy : 0.880440\n","total step : 410 \n","error : 0.410558, accuarcy : 0.880940\n","total step : 411 \n","error : 0.410104, accuarcy : 0.880940\n","total step : 412 \n","error : 0.409652, accuarcy : 0.880940\n","total step : 413 \n","error : 0.409201, accuarcy : 0.880940\n","total step : 414 \n","error : 0.408752, accuarcy : 0.881941\n","total step : 415 \n","error : 0.408305, accuarcy : 0.882441\n","total step : 416 \n","error : 0.407858, accuarcy : 0.882441\n","total step : 417 \n","error : 0.407414, accuarcy : 0.882441\n","total step : 418 \n","error : 0.406971, accuarcy : 0.882441\n","total step : 419 \n","error : 0.406529, accuarcy : 0.882441\n","total step : 420 \n","error : 0.406089, accuarcy : 0.882441\n","total step : 421 \n","error : 0.405650, accuarcy : 0.882441\n","total step : 422 \n","error : 0.405213, accuarcy : 0.882941\n","total step : 423 \n","error : 0.404777, accuarcy : 0.882941\n","total step : 424 \n","error : 0.404343, accuarcy : 0.882941\n","total step : 425 \n","error : 0.403910, accuarcy : 0.883442\n","total step : 426 \n","error : 0.403478, accuarcy : 0.883442\n","total step : 427 \n","error : 0.403048, accuarcy : 0.883442\n","total step : 428 \n","error : 0.402619, accuarcy : 0.883442\n","total step : 429 \n","error : 0.402192, accuarcy : 0.883942\n","total step : 430 \n","error : 0.401766, accuarcy : 0.883942\n","total step : 431 \n","error : 0.401342, accuarcy : 0.883942\n","total step : 432 \n","error : 0.400918, accuarcy : 0.883942\n","total step : 433 \n","error : 0.400497, accuarcy : 0.883942\n","total step : 434 \n","error : 0.400076, accuarcy : 0.884442\n","total step : 435 \n","error : 0.399657, accuarcy : 0.884942\n","total step : 436 \n","error : 0.399239, accuarcy : 0.885443\n","total step : 437 \n","error : 0.398823, accuarcy : 0.885443\n","total step : 438 \n","error : 0.398408, accuarcy : 0.885443\n","total step : 439 \n","error : 0.397994, accuarcy : 0.885943\n","total step : 440 \n","error : 0.397582, accuarcy : 0.885443\n","total step : 441 \n","error : 0.397170, accuarcy : 0.886443\n","total step : 442 \n","error : 0.396761, accuarcy : 0.886443\n","total step : 443 \n","error : 0.396352, accuarcy : 0.886443\n","total step : 444 \n","error : 0.395945, accuarcy : 0.885943\n","total step : 445 \n","error : 0.395539, accuarcy : 0.885943\n","total step : 446 \n","error : 0.395134, accuarcy : 0.885943\n","total step : 447 \n","error : 0.394731, accuarcy : 0.885943\n","total step : 448 \n","error : 0.394329, accuarcy : 0.885943\n","total step : 449 \n","error : 0.393928, accuarcy : 0.885943\n","total step : 450 \n","error : 0.393528, accuarcy : 0.885943\n","total step : 451 \n","error : 0.393130, accuarcy : 0.885943\n","total step : 452 \n","error : 0.392733, accuarcy : 0.885943\n","total step : 453 \n","error : 0.392337, accuarcy : 0.885943\n","total step : 454 \n","error : 0.391942, accuarcy : 0.885943\n","total step : 455 \n","error : 0.391549, accuarcy : 0.885943\n","total step : 456 \n","error : 0.391156, accuarcy : 0.885943\n","total step : 457 \n","error : 0.390765, accuarcy : 0.885943\n","total step : 458 \n","error : 0.390376, accuarcy : 0.886443\n","total step : 459 \n","error : 0.389987, accuarcy : 0.886443\n","total step : 460 \n","error : 0.389600, accuarcy : 0.886443\n","total step : 461 \n","error : 0.389213, accuarcy : 0.886443\n","total step : 462 \n","error : 0.388828, accuarcy : 0.886443\n","total step : 463 \n","error : 0.388444, accuarcy : 0.886443\n","total step : 464 \n","error : 0.388062, accuarcy : 0.886943\n","total step : 465 \n","error : 0.387680, accuarcy : 0.886943\n","total step : 466 \n","error : 0.387300, accuarcy : 0.886943\n","total step : 467 \n","error : 0.386920, accuarcy : 0.886943\n","total step : 468 \n","error : 0.386542, accuarcy : 0.886943\n","total step : 469 \n","error : 0.386165, accuarcy : 0.886943\n","total step : 470 \n","error : 0.385790, accuarcy : 0.886943\n","total step : 471 \n","error : 0.385415, accuarcy : 0.886943\n","total step : 472 \n","error : 0.385041, accuarcy : 0.886943\n","total step : 473 \n","error : 0.384669, accuarcy : 0.886943\n","total step : 474 \n","error : 0.384297, accuarcy : 0.886943\n","total step : 475 \n","error : 0.383927, accuarcy : 0.886943\n","total step : 476 \n","error : 0.383558, accuarcy : 0.886943\n","total step : 477 \n","error : 0.383190, accuarcy : 0.886943\n","total step : 478 \n","error : 0.382823, accuarcy : 0.886943\n","total step : 479 \n","error : 0.382457, accuarcy : 0.886943\n","total step : 480 \n","error : 0.382092, accuarcy : 0.886943\n","total step : 481 \n","error : 0.381729, accuarcy : 0.886943\n","total step : 482 \n","error : 0.381366, accuarcy : 0.887444\n","total step : 483 \n","error : 0.381005, accuarcy : 0.887444\n","total step : 484 \n","error : 0.380644, accuarcy : 0.888444\n","total step : 485 \n","error : 0.380285, accuarcy : 0.888444\n","total step : 486 \n","error : 0.379926, accuarcy : 0.887944\n","total step : 487 \n","error : 0.379569, accuarcy : 0.887944\n","total step : 488 \n","error : 0.379212, accuarcy : 0.887944\n","total step : 489 \n","error : 0.378857, accuarcy : 0.887944\n","total step : 490 \n","error : 0.378503, accuarcy : 0.887444\n","total step : 491 \n","error : 0.378150, accuarcy : 0.887444\n","total step : 492 \n","error : 0.377798, accuarcy : 0.887444\n","total step : 493 \n","error : 0.377446, accuarcy : 0.887944\n","total step : 494 \n","error : 0.377096, accuarcy : 0.887944\n","total step : 495 \n","error : 0.376747, accuarcy : 0.887944\n","total step : 496 \n","error : 0.376399, accuarcy : 0.888444\n","total step : 497 \n","error : 0.376052, accuarcy : 0.888444\n","total step : 498 \n","error : 0.375706, accuarcy : 0.888444\n","total step : 499 \n","error : 0.375360, accuarcy : 0.888444\n","total step : 500 \n","error : 0.375016, accuarcy : 0.888944\n","total step : 501 \n","error : 0.374673, accuarcy : 0.888944\n","total step : 502 \n","error : 0.374331, accuarcy : 0.888944\n","total step : 503 \n","error : 0.373990, accuarcy : 0.888944\n","total step : 504 \n","error : 0.373649, accuarcy : 0.888944\n","total step : 505 \n","error : 0.373310, accuarcy : 0.889445\n","total step : 506 \n","error : 0.372972, accuarcy : 0.889445\n","total step : 507 \n","error : 0.372634, accuarcy : 0.889445\n","total step : 508 \n","error : 0.372298, accuarcy : 0.889445\n","total step : 509 \n","error : 0.371962, accuarcy : 0.889445\n","total step : 510 \n","error : 0.371628, accuarcy : 0.889445\n","total step : 511 \n","error : 0.371294, accuarcy : 0.889445\n","total step : 512 \n","error : 0.370961, accuarcy : 0.889445\n","total step : 513 \n","error : 0.370629, accuarcy : 0.889445\n","total step : 514 \n","error : 0.370299, accuarcy : 0.889945\n","total step : 515 \n","error : 0.369969, accuarcy : 0.889945\n","total step : 516 \n","error : 0.369640, accuarcy : 0.890445\n","total step : 517 \n","error : 0.369312, accuarcy : 0.890445\n","total step : 518 \n","error : 0.368984, accuarcy : 0.890445\n","total step : 519 \n","error : 0.368658, accuarcy : 0.890445\n","total step : 520 \n","error : 0.368333, accuarcy : 0.890445\n","total step : 521 \n","error : 0.368008, accuarcy : 0.890445\n","total step : 522 \n","error : 0.367685, accuarcy : 0.890945\n","total step : 523 \n","error : 0.367362, accuarcy : 0.890945\n","total step : 524 \n","error : 0.367040, accuarcy : 0.890945\n","total step : 525 \n","error : 0.366719, accuarcy : 0.891446\n","total step : 526 \n","error : 0.366399, accuarcy : 0.891446\n","total step : 527 \n","error : 0.366080, accuarcy : 0.891446\n","total step : 528 \n","error : 0.365762, accuarcy : 0.891446\n","total step : 529 \n","error : 0.365444, accuarcy : 0.891446\n","total step : 530 \n","error : 0.365128, accuarcy : 0.891446\n","total step : 531 \n","error : 0.364812, accuarcy : 0.891446\n","total step : 532 \n","error : 0.364497, accuarcy : 0.891446\n","total step : 533 \n","error : 0.364183, accuarcy : 0.891446\n","total step : 534 \n","error : 0.363870, accuarcy : 0.891446\n","total step : 535 \n","error : 0.363558, accuarcy : 0.890945\n","total step : 536 \n","error : 0.363246, accuarcy : 0.890945\n","total step : 537 \n","error : 0.362936, accuarcy : 0.890945\n","total step : 538 \n","error : 0.362626, accuarcy : 0.890945\n","total step : 539 \n","error : 0.362317, accuarcy : 0.890945\n","total step : 540 \n","error : 0.362009, accuarcy : 0.890945\n","total step : 541 \n","error : 0.361701, accuarcy : 0.890945\n","total step : 542 \n","error : 0.361395, accuarcy : 0.890945\n","total step : 543 \n","error : 0.361089, accuarcy : 0.890945\n","total step : 544 \n","error : 0.360784, accuarcy : 0.890945\n","total step : 545 \n","error : 0.360480, accuarcy : 0.890945\n","total step : 546 \n","error : 0.360177, accuarcy : 0.890945\n","total step : 547 \n","error : 0.359875, accuarcy : 0.890945\n","total step : 548 \n","error : 0.359573, accuarcy : 0.890945\n","total step : 549 \n","error : 0.359272, accuarcy : 0.890945\n","total step : 550 \n","error : 0.358972, accuarcy : 0.890945\n","total step : 551 \n","error : 0.358673, accuarcy : 0.890945\n","total step : 552 \n","error : 0.358374, accuarcy : 0.890945\n","total step : 553 \n","error : 0.358077, accuarcy : 0.891446\n","total step : 554 \n","error : 0.357780, accuarcy : 0.891446\n","total step : 555 \n","error : 0.357484, accuarcy : 0.891446\n","total step : 556 \n","error : 0.357188, accuarcy : 0.891446\n","total step : 557 \n","error : 0.356894, accuarcy : 0.891946\n","total step : 558 \n","error : 0.356600, accuarcy : 0.892446\n","total step : 559 \n","error : 0.356307, accuarcy : 0.891946\n","total step : 560 \n","error : 0.356015, accuarcy : 0.891946\n","total step : 561 \n","error : 0.355723, accuarcy : 0.891946\n","total step : 562 \n","error : 0.355432, accuarcy : 0.891946\n","total step : 563 \n","error : 0.355142, accuarcy : 0.891946\n","total step : 564 \n","error : 0.354853, accuarcy : 0.891946\n","total step : 565 \n","error : 0.354565, accuarcy : 0.891946\n","total step : 566 \n","error : 0.354277, accuarcy : 0.891946\n","total step : 567 \n","error : 0.353990, accuarcy : 0.891946\n","total step : 568 \n","error : 0.353703, accuarcy : 0.891946\n","total step : 569 \n","error : 0.353418, accuarcy : 0.891946\n","total step : 570 \n","error : 0.353133, accuarcy : 0.892446\n","total step : 571 \n","error : 0.352849, accuarcy : 0.892946\n","total step : 572 \n","error : 0.352566, accuarcy : 0.892946\n","total step : 573 \n","error : 0.352283, accuarcy : 0.892946\n","total step : 574 \n","error : 0.352001, accuarcy : 0.892946\n","total step : 575 \n","error : 0.351720, accuarcy : 0.892946\n","total step : 576 \n","error : 0.351439, accuarcy : 0.892946\n","total step : 577 \n","error : 0.351160, accuarcy : 0.893947\n","total step : 578 \n","error : 0.350881, accuarcy : 0.893947\n","total step : 579 \n","error : 0.350602, accuarcy : 0.893947\n","total step : 580 \n","error : 0.350325, accuarcy : 0.893947\n","total step : 581 \n","error : 0.350048, accuarcy : 0.893947\n","total step : 582 \n","error : 0.349771, accuarcy : 0.893947\n","total step : 583 \n","error : 0.349496, accuarcy : 0.893947\n","total step : 584 \n","error : 0.349221, accuarcy : 0.893947\n","total step : 585 \n","error : 0.348947, accuarcy : 0.893947\n","total step : 586 \n","error : 0.348673, accuarcy : 0.893947\n","total step : 587 \n","error : 0.348401, accuarcy : 0.893947\n","total step : 588 \n","error : 0.348128, accuarcy : 0.893447\n","total step : 589 \n","error : 0.347857, accuarcy : 0.893447\n","total step : 590 \n","error : 0.347586, accuarcy : 0.893447\n","total step : 591 \n","error : 0.347316, accuarcy : 0.893947\n","total step : 592 \n","error : 0.347047, accuarcy : 0.893947\n","total step : 593 \n","error : 0.346778, accuarcy : 0.894447\n","total step : 594 \n","error : 0.346510, accuarcy : 0.894447\n","total step : 595 \n","error : 0.346243, accuarcy : 0.894447\n","total step : 596 \n","error : 0.345976, accuarcy : 0.894447\n","total step : 597 \n","error : 0.345710, accuarcy : 0.894447\n","total step : 598 \n","error : 0.345444, accuarcy : 0.894447\n","total step : 599 \n","error : 0.345180, accuarcy : 0.894447\n","total step : 600 \n","error : 0.344915, accuarcy : 0.894447\n","total step : 601 \n","error : 0.344652, accuarcy : 0.894447\n","total step : 602 \n","error : 0.344389, accuarcy : 0.894447\n","total step : 603 \n","error : 0.344127, accuarcy : 0.894447\n","total step : 604 \n","error : 0.343865, accuarcy : 0.894447\n","total step : 605 \n","error : 0.343605, accuarcy : 0.894447\n","total step : 606 \n","error : 0.343344, accuarcy : 0.894447\n","total step : 607 \n","error : 0.343085, accuarcy : 0.894447\n","total step : 608 \n","error : 0.342826, accuarcy : 0.894447\n","total step : 609 \n","error : 0.342567, accuarcy : 0.894447\n","total step : 610 \n","error : 0.342310, accuarcy : 0.894447\n","total step : 611 \n","error : 0.342053, accuarcy : 0.894447\n","total step : 612 \n","error : 0.341796, accuarcy : 0.894447\n","total step : 613 \n","error : 0.341540, accuarcy : 0.894447\n","total step : 614 \n","error : 0.341285, accuarcy : 0.894447\n","total step : 615 \n","error : 0.341030, accuarcy : 0.894447\n","total step : 616 \n","error : 0.340776, accuarcy : 0.894447\n","total step : 617 \n","error : 0.340523, accuarcy : 0.894447\n","total step : 618 \n","error : 0.340270, accuarcy : 0.894447\n","total step : 619 \n","error : 0.340018, accuarcy : 0.894447\n","total step : 620 \n","error : 0.339766, accuarcy : 0.894447\n","total step : 621 \n","error : 0.339516, accuarcy : 0.894447\n","total step : 622 \n","error : 0.339265, accuarcy : 0.894447\n","total step : 623 \n","error : 0.339015, accuarcy : 0.894447\n","total step : 624 \n","error : 0.338766, accuarcy : 0.894447\n","total step : 625 \n","error : 0.338518, accuarcy : 0.894447\n","total step : 626 \n","error : 0.338270, accuarcy : 0.894447\n","total step : 627 \n","error : 0.338022, accuarcy : 0.894947\n","total step : 628 \n","error : 0.337776, accuarcy : 0.894947\n","total step : 629 \n","error : 0.337529, accuarcy : 0.894947\n","total step : 630 \n","error : 0.337284, accuarcy : 0.895448\n","total step : 631 \n","error : 0.337039, accuarcy : 0.895448\n","total step : 632 \n","error : 0.336794, accuarcy : 0.895448\n","total step : 633 \n","error : 0.336550, accuarcy : 0.895448\n","total step : 634 \n","error : 0.336307, accuarcy : 0.895448\n","total step : 635 \n","error : 0.336064, accuarcy : 0.895448\n","total step : 636 \n","error : 0.335822, accuarcy : 0.895448\n","total step : 637 \n","error : 0.335580, accuarcy : 0.895448\n","total step : 638 \n","error : 0.335339, accuarcy : 0.895948\n","total step : 639 \n","error : 0.335099, accuarcy : 0.895948\n","total step : 640 \n","error : 0.334859, accuarcy : 0.895948\n","total step : 641 \n","error : 0.334620, accuarcy : 0.895948\n","total step : 642 \n","error : 0.334381, accuarcy : 0.895948\n","total step : 643 \n","error : 0.334143, accuarcy : 0.896448\n","total step : 644 \n","error : 0.333905, accuarcy : 0.896448\n","total step : 645 \n","error : 0.333668, accuarcy : 0.896448\n","total step : 646 \n","error : 0.333431, accuarcy : 0.896448\n","total step : 647 \n","error : 0.333195, accuarcy : 0.896948\n","total step : 648 \n","error : 0.332960, accuarcy : 0.896948\n","total step : 649 \n","error : 0.332725, accuarcy : 0.896948\n","total step : 650 \n","error : 0.332491, accuarcy : 0.896948\n","total step : 651 \n","error : 0.332257, accuarcy : 0.897449\n","total step : 652 \n","error : 0.332024, accuarcy : 0.896948\n","total step : 653 \n","error : 0.331791, accuarcy : 0.896948\n","total step : 654 \n","error : 0.331559, accuarcy : 0.896948\n","total step : 655 \n","error : 0.331327, accuarcy : 0.896948\n","total step : 656 \n","error : 0.331096, accuarcy : 0.896948\n","total step : 657 \n","error : 0.330865, accuarcy : 0.896448\n","total step : 658 \n","error : 0.330635, accuarcy : 0.896448\n","total step : 659 \n","error : 0.330405, accuarcy : 0.896448\n","total step : 660 \n","error : 0.330176, accuarcy : 0.896448\n","total step : 661 \n","error : 0.329948, accuarcy : 0.897449\n","total step : 662 \n","error : 0.329720, accuarcy : 0.897449\n","total step : 663 \n","error : 0.329492, accuarcy : 0.897449\n","total step : 664 \n","error : 0.329266, accuarcy : 0.897949\n","total step : 665 \n","error : 0.329039, accuarcy : 0.897949\n","total step : 666 \n","error : 0.328813, accuarcy : 0.897949\n","total step : 667 \n","error : 0.328588, accuarcy : 0.898449\n","total step : 668 \n","error : 0.328363, accuarcy : 0.898449\n","total step : 669 \n","error : 0.328138, accuarcy : 0.898449\n","total step : 670 \n","error : 0.327915, accuarcy : 0.898449\n","total step : 671 \n","error : 0.327691, accuarcy : 0.898449\n","total step : 672 \n","error : 0.327468, accuarcy : 0.898449\n","total step : 673 \n","error : 0.327246, accuarcy : 0.898949\n","total step : 674 \n","error : 0.327024, accuarcy : 0.898949\n","total step : 675 \n","error : 0.326803, accuarcy : 0.898949\n","total step : 676 \n","error : 0.326582, accuarcy : 0.898949\n","total step : 677 \n","error : 0.326362, accuarcy : 0.898949\n","total step : 678 \n","error : 0.326142, accuarcy : 0.898949\n","total step : 679 \n","error : 0.325922, accuarcy : 0.898949\n","total step : 680 \n","error : 0.325703, accuarcy : 0.898949\n","total step : 681 \n","error : 0.325485, accuarcy : 0.898949\n","total step : 682 \n","error : 0.325267, accuarcy : 0.898949\n","total step : 683 \n","error : 0.325050, accuarcy : 0.898949\n","total step : 684 \n","error : 0.324833, accuarcy : 0.898449\n","total step : 685 \n","error : 0.324616, accuarcy : 0.898449\n","total step : 686 \n","error : 0.324400, accuarcy : 0.898449\n","total step : 687 \n","error : 0.324185, accuarcy : 0.898449\n","total step : 688 \n","error : 0.323970, accuarcy : 0.898449\n","total step : 689 \n","error : 0.323755, accuarcy : 0.898449\n","total step : 690 \n","error : 0.323541, accuarcy : 0.898449\n","total step : 691 \n","error : 0.323327, accuarcy : 0.898449\n","total step : 692 \n","error : 0.323114, accuarcy : 0.898449\n","total step : 693 \n","error : 0.322902, accuarcy : 0.898949\n","total step : 694 \n","error : 0.322689, accuarcy : 0.898949\n","total step : 695 \n","error : 0.322478, accuarcy : 0.898949\n","total step : 696 \n","error : 0.322266, accuarcy : 0.898949\n","total step : 697 \n","error : 0.322056, accuarcy : 0.898949\n","total step : 698 \n","error : 0.321845, accuarcy : 0.898949\n","total step : 699 \n","error : 0.321635, accuarcy : 0.898949\n","total step : 700 \n","error : 0.321426, accuarcy : 0.898949\n","total step : 701 \n","error : 0.321217, accuarcy : 0.898949\n","total step : 702 \n","error : 0.321009, accuarcy : 0.898449\n","total step : 703 \n","error : 0.320801, accuarcy : 0.898449\n","total step : 704 \n","error : 0.320593, accuarcy : 0.898449\n","total step : 705 \n","error : 0.320386, accuarcy : 0.898949\n","total step : 706 \n","error : 0.320179, accuarcy : 0.898949\n","total step : 707 \n","error : 0.319973, accuarcy : 0.898949\n","total step : 708 \n","error : 0.319767, accuarcy : 0.898949\n","total step : 709 \n","error : 0.319562, accuarcy : 0.898949\n","total step : 710 \n","error : 0.319357, accuarcy : 0.898949\n","total step : 711 \n","error : 0.319152, accuarcy : 0.899450\n","total step : 712 \n","error : 0.318948, accuarcy : 0.899450\n","total step : 713 \n","error : 0.318745, accuarcy : 0.898949\n","total step : 714 \n","error : 0.318542, accuarcy : 0.898949\n","total step : 715 \n","error : 0.318339, accuarcy : 0.898949\n","total step : 716 \n","error : 0.318137, accuarcy : 0.898949\n","total step : 717 \n","error : 0.317935, accuarcy : 0.898949\n","total step : 718 \n","error : 0.317733, accuarcy : 0.898949\n","total step : 719 \n","error : 0.317532, accuarcy : 0.898949\n","total step : 720 \n","error : 0.317332, accuarcy : 0.898949\n","total step : 721 \n","error : 0.317132, accuarcy : 0.898949\n","total step : 722 \n","error : 0.316932, accuarcy : 0.898949\n","total step : 723 \n","error : 0.316733, accuarcy : 0.898949\n","total step : 724 \n","error : 0.316534, accuarcy : 0.898949\n","total step : 725 \n","error : 0.316336, accuarcy : 0.898949\n","total step : 726 \n","error : 0.316138, accuarcy : 0.898949\n","total step : 727 \n","error : 0.315940, accuarcy : 0.898949\n","total step : 728 \n","error : 0.315743, accuarcy : 0.898949\n","total step : 729 \n","error : 0.315546, accuarcy : 0.898949\n","total step : 730 \n","error : 0.315350, accuarcy : 0.898949\n","total step : 731 \n","error : 0.315154, accuarcy : 0.898949\n","total step : 732 \n","error : 0.314958, accuarcy : 0.899450\n","total step : 733 \n","error : 0.314763, accuarcy : 0.899450\n","total step : 734 \n","error : 0.314569, accuarcy : 0.899950\n","total step : 735 \n","error : 0.314374, accuarcy : 0.899950\n","total step : 736 \n","error : 0.314181, accuarcy : 0.899950\n","total step : 737 \n","error : 0.313987, accuarcy : 0.899950\n","total step : 738 \n","error : 0.313794, accuarcy : 0.899950\n","total step : 739 \n","error : 0.313602, accuarcy : 0.900450\n","\n","L2_total step : 379 \n","L2_error : 1.932102, L2_accuarcy : 0.900450\n","loss :  8.063079315186755\n","L2_loss :  9.72686192514028\n"]}],"source":["def L2_regularization(w, lamb) :  # L2 Regularization 적용한 w 반환\n","    return lamb * np.sum(w**2)\n","\n","def CrossEntropyLoss_L2(preds, y, w, lamb) :\n","    delta = 1e-7\n","    loss = np.sum(-y*np.log(preds+delta)-(1-y)*np.log(1-preds+delta))/len(preds) + L2_regularization(w, lamb) # 기존 loss의 L2 정규화 적용용\n","    return loss\n","\n","def train_L2(X, y, lamb) :\n","    w = np.random.randn(len(X[0]), 1) # weight initialization\n","    lr = 0.1\n","    step = 0\n","    acc = 0\n","\n","    while (acc <= 0.9) :  # 좀 더 나은 정확성을 위해 acc이이 0.9 이상일 때 반복문 탈출\n","        step += 1\n","        \n","        # predict\n","        preds = 1 / (1 + np.exp(-np.dot(X, w)))\n","        loss = CrossEntropyLoss_L2(preds, y, w, lamb) # L2 정규화 적용된 loss를 받는다.\n","        \n","        result = np.where(preds>0.5, 1, 0)\n","        acc = np.sum(np.where(result==y, True, False))/len(preds)\n","        \n","       \n","        \n","        # gradient descent\n","        gradient = np.dot(X.T, (preds - y)) / len(preds) + 2 * lamb * w\n","        w -= lr * gradient\n","\n","    # 마지막 값만 도출출\n","    print(\"\\nL2_total step : %d \" % step)\n","    print(\"L2_error : %f, L2_accuarcy : %f\" % (loss, acc))\n","\n","    return w\n","\n","org_w = train(train_X, train_y)\n","l2_w = train_L2(train_X, train_y, 0.01)\n","\n","org_loss = CrossEntropyLoss(np.zeros((len(train_X), 1)), train_y)\n","print('loss : ', org_loss)\n","\n","l2_loss =  CrossEntropyLoss_L2(np.zeros((len(train_X), 1)), train_y, l2_w, 0.01)\n","print('L2_loss : ', l2_loss)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["L2 Regularization이 적용된 후 는 적용 전에 비해 Accuracy에 도달하는 과정에서 error와 loss가 크게 나오지만, total step은 적게 나온다."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["이러한 이유로는 L2 Regularization을 적용하면 손실 함수에 정규화 항이 추가되어 손실 값이 커져 가중치의 크기를 제한하는 역할을 하기 때문이다."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["lamb 값 역시 중요하다. lamb 값이 너무 크다면 모델이 데이터에 충분히 적합하지 못하게 되고, 너무 작다면 정규화의 효과를 보지 못한다"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"ml","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"ac989702ee82cf85834fc904c407af640e06b0902e988e9cd8a067d885b3b790"}}},"nbformat":4,"nbformat_minor":0}
