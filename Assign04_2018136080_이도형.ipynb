{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1680938076599,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"qQB-kQer4fnL"},"outputs":[],"source":["import numpy as np \n","import matplotlib.pyplot as plt\n","\n","# install\n","## numpy\n","## matplotlib"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22815,"status":"ok","timestamp":1680938099410,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"1vHd_0aq42uH","outputId":"48d13b00-2490-4df0-e165-c8859a96919d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content\n","/content/drive/MyDrive/Colab Notebooks/assign04\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pwd\n","%cd /content/drive/MyDrive/Colab Notebooks/assign04"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VVEidagk4fnO"},"source":["## data load & preprocessing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 손글씨 데이터셋"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2072,"status":"ok","timestamp":1680938101480,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"sreJ921r4fnP","outputId":"95592ead-b7af-4f12-8259-47e3a9c41993"},"outputs":[{"name":"stdout","output_type":"stream","text":["(60000, 1, 28, 28)\n"]}],"source":["from dataset.mnist import load_mnist\n","\n","(train_raw_img, train_label), (test_raw_img, test_label) = load_mnist(flatten=False, normalize=False)\n","print(train_raw_img.shape)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680938101481,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"NnXkrHd_4fnP","outputId":"3cfb7db6-55d4-4bbc-f356-a374d9aa0284"},"outputs":[{"name":"stdout","output_type":"stream","text":["(60000, 784)\n","(60000, 1)\n","(10000, 784)\n","(10000, 1)\n"]}],"source":["# preprocessing (train & inference)\n","\n","train_img = train_raw_img.reshape(len(train_raw_img.squeeze()), -1)\n","train_label = train_label.reshape(len(train_label), -1)\n","\n","test_img = test_raw_img.reshape(len(test_raw_img.squeeze()), -1)\n","test_label = test_label.reshape(len(test_label), -1)\n","\n","print(train_img.shape)\n","print(train_label.shape)\n","print(test_img.shape)\n","print(test_label.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680938101481,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"i0yWI_RK4fnQ"},"outputs":[],"source":["# normalization (set value 0 ~ 1)\n","\n","train_img = train_img.astype('float')\n","train_img = train_img/255\n","\n","test_img = test_img.astype('float')\n","test_img = test_img/255"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Softmax Regression for Multi Class Single-label Classfication"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7TCKNAlG4fnQ"},"source":["##### 지난 시간 Classification 결과를 Sigmoid 대신 Softmax로 학습한다."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["data -> Linear(784, 100) -> ReLU(100, 100) -> Linear(100, 10) -> softmax(10, 10)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680938101481,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"SrJxQcyS4fnQ"},"outputs":[],"source":["class Linear :\n","    def __init__(self, input_size=1, hidden_size=1) :\n","        self.W = np.random.randn(input_size, hidden_size) # input_size와 hidden_size를 가지는 정규분포를 따르는 난수 생성\n","        self.b = np.zeros(hidden_size)\n","        self.x = None\n","        self.dW = None\n","        self.db = None\n","    \n","    def forward(self, x) :\n","        self.x = x\n","        out = np.dot(x, self.W) + self.b  # 입력값(x) * 가중치(W) + Bias(b)\n","        return out\n","    \n","    def backward(self, dout, lr) :\n","        dx = np.dot(dout, self.W.T) # 입력값에 대한 미분값\n","        self.dW = np.dot(self.x.T, dout)  # 가중치에 대한 미분값\n","        self.db = np.sum(dout, axis=0)  # Bias에 대한 미분값\n","        self.W -= lr * self.dW  # 학습률(lr)을 통해 가중치 업데이트\n","        self.b -= lr * self.db  # 학습률(lr)을 통해 Bias 업데이트\n","        return dx"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680938101482,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"ngiCPY164fnR"},"outputs":[],"source":["class Relu :\n","    def __init__(self) :\n","        self.mask = None\n","    \n","    def forward(self, x) :\n","        self.mask = (x < 0)\n","        out = x.copy()\n","        out[self.mask] = 0\n","        return out\n","    \n","    def backward(self, dout) :\n","        dout[self.mask] = 0\n","        return dout                                                                                                                                                                                                                                                                                                                                                   "]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680938101482,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"cxaKGgVm4fnR"},"outputs":[],"source":["class softmax_with_crossEntropy :\n","    def __init__(self) :\n","        self.delta = 1e-7\n","        # softmax\n","        self.softmax_x = None\n","        self.softmax_out = None\n","        # crossEntropy\n","        self.pred = None\n","        self.target = None\n","        \n","    def softmax_forward(self, x) :\n","        self.softmax_x = x  # 입력값 x를 softmax_x에 입력\n","        max_x = np.max(x, axis=1, keepdims=True)\n","        exp_x = np.exp(x - max_x) # 입력값에서 최대값을 빼주어 지수함수 계산\n","        self.softmax_out = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n","        return self.softmax_out # softmax 함수의 출력 값\n","    \n","    def crossEntropy_forward(self, pred, target) :\n","        self.pred = pred  # 모델이 예측한 값 (pred)\n","        self.target = target  # 실제값 (target)\n","        batch_size = pred.shape[0]\n","        loss = -np.sum(np.log(pred + self.delta) * target) / batch_size\n","        return loss\n","    \n","    def backward(self) :\n","        batch_size = self.target.shape[0]\n","        dx = (self.pred - self.target) / batch_size # Cross Entropy 함수의 gradient 계산\n","        return dx"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"BUWjwbvO4fnR"},"source":["## Train"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"C746dsF24fnS"},"source":["### base"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680938101482,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"HdUXsNlN4fnS"},"outputs":[],"source":["# one_hot label 만드는 함수\n","\n","def make_one_hot(labels) :\n","    a = []\n","    for label in labels :\n","        one_hot = np.zeros(10)\n","        one_hot[label] = 1\n","        a.append(one_hot)\n","    a = np.array(a)\n","    return a\n","\n","# one_hot_labels = make_one_hot(train_label)\n","# print(train_label[0])\n","# print(one_hot_labels[0])"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1680938101482,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"U5jmqwkB4fnS"},"outputs":[],"source":["# train version 1\n","\n","from collections import OrderedDict\n","\n","def train_MLP(config) :\n","    lr, num_epoch = config['learning_rate'], config['num_epoch']\n","    print_loss_interval = 1\n","    \n","    layer1 = Linear(784, 100)\n","    relu = Relu()\n","    layer2 = Linear(100, 10)\n","    softmax_with_CE = softmax_with_crossEntropy()\n","    \n","    for iter in range(num_epoch) :\n","        # forward\n","        x = layer1.forward(train_img)\n","        x = relu.forward(x)\n","        x = layer2.forward(x)\n","        preds = softmax_with_CE.softmax_forward(x)\n","        \n","        # loss\n","        one_hot_labels = make_one_hot(train_label)\n","        losses = softmax_with_CE.crossEntropy_forward(preds, one_hot_labels)\n","        loss = losses.sum()/len(preds)\n","        \n","        if iter % print_loss_interval == 0:\n","            print(\"[epoch %d / %d] average loss : %f\" % (iter, num_epoch, loss))\n","        \n","        # backward\n","        dL = softmax_with_CE.backward()\n","        dL = layer2.backward(dL, lr)\n","        dL = relu.backward(dL)\n","        dL = layer1.backward(dL, lr)\n","\n","    model = OrderedDict()\n","    model['layer1'] = layer1\n","    model['relu'] = relu\n","    model['layer2'] = layer2\n","    model['softmax_with_CE'] = softmax_with_CE\n","    \n","    return model"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":136004,"status":"ok","timestamp":1680938237482,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"ngujcVaB4fnS","outputId":"c65dbbd8-80fb-4bc3-f277-06c5f276ab2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[epoch 0 / 100] average loss : 0.000244\n","[epoch 1 / 100] average loss : 0.000240\n","[epoch 2 / 100] average loss : 0.000224\n","[epoch 3 / 100] average loss : 0.000211\n","[epoch 4 / 100] average loss : 0.000199\n","[epoch 5 / 100] average loss : 0.000189\n","[epoch 6 / 100] average loss : 0.000179\n","[epoch 7 / 100] average loss : 0.000170\n","[epoch 8 / 100] average loss : 0.000162\n","[epoch 9 / 100] average loss : 0.000154\n","[epoch 10 / 100] average loss : 0.000148\n","[epoch 11 / 100] average loss : 0.000141\n","[epoch 12 / 100] average loss : 0.000136\n","[epoch 13 / 100] average loss : 0.000131\n","[epoch 14 / 100] average loss : 0.000126\n","[epoch 15 / 100] average loss : 0.000122\n","[epoch 16 / 100] average loss : 0.000118\n","[epoch 17 / 100] average loss : 0.000114\n","[epoch 18 / 100] average loss : 0.000111\n","[epoch 19 / 100] average loss : 0.000108\n","[epoch 20 / 100] average loss : 0.000105\n","[epoch 21 / 100] average loss : 0.000102\n","[epoch 22 / 100] average loss : 0.000100\n","[epoch 23 / 100] average loss : 0.000098\n","[epoch 24 / 100] average loss : 0.000095\n","[epoch 25 / 100] average loss : 0.000093\n","[epoch 26 / 100] average loss : 0.000091\n","[epoch 27 / 100] average loss : 0.000090\n","[epoch 28 / 100] average loss : 0.000088\n","[epoch 29 / 100] average loss : 0.000086\n","[epoch 30 / 100] average loss : 0.000085\n","[epoch 31 / 100] average loss : 0.000083\n","[epoch 32 / 100] average loss : 0.000082\n","[epoch 33 / 100] average loss : 0.000080\n","[epoch 34 / 100] average loss : 0.000079\n","[epoch 35 / 100] average loss : 0.000078\n","[epoch 36 / 100] average loss : 0.000077\n","[epoch 37 / 100] average loss : 0.000075\n","[epoch 38 / 100] average loss : 0.000074\n","[epoch 39 / 100] average loss : 0.000073\n","[epoch 40 / 100] average loss : 0.000072\n","[epoch 41 / 100] average loss : 0.000071\n","[epoch 42 / 100] average loss : 0.000070\n","[epoch 43 / 100] average loss : 0.000069\n","[epoch 44 / 100] average loss : 0.000069\n","[epoch 45 / 100] average loss : 0.000068\n","[epoch 46 / 100] average loss : 0.000067\n","[epoch 47 / 100] average loss : 0.000066\n","[epoch 48 / 100] average loss : 0.000065\n","[epoch 49 / 100] average loss : 0.000065\n","[epoch 50 / 100] average loss : 0.000064\n","[epoch 51 / 100] average loss : 0.000063\n","[epoch 52 / 100] average loss : 0.000063\n","[epoch 53 / 100] average loss : 0.000062\n","[epoch 54 / 100] average loss : 0.000061\n","[epoch 55 / 100] average loss : 0.000061\n","[epoch 56 / 100] average loss : 0.000060\n","[epoch 57 / 100] average loss : 0.000059\n","[epoch 58 / 100] average loss : 0.000059\n","[epoch 59 / 100] average loss : 0.000058\n","[epoch 60 / 100] average loss : 0.000058\n","[epoch 61 / 100] average loss : 0.000057\n","[epoch 62 / 100] average loss : 0.000057\n","[epoch 63 / 100] average loss : 0.000056\n","[epoch 64 / 100] average loss : 0.000056\n","[epoch 65 / 100] average loss : 0.000055\n","[epoch 66 / 100] average loss : 0.000055\n","[epoch 67 / 100] average loss : 0.000054\n","[epoch 68 / 100] average loss : 0.000054\n","[epoch 69 / 100] average loss : 0.000054\n","[epoch 70 / 100] average loss : 0.000053\n","[epoch 71 / 100] average loss : 0.000053\n","[epoch 72 / 100] average loss : 0.000052\n","[epoch 73 / 100] average loss : 0.000052\n","[epoch 74 / 100] average loss : 0.000052\n","[epoch 75 / 100] average loss : 0.000051\n","[epoch 76 / 100] average loss : 0.000051\n","[epoch 77 / 100] average loss : 0.000051\n","[epoch 78 / 100] average loss : 0.000050\n","[epoch 79 / 100] average loss : 0.000050\n","[epoch 80 / 100] average loss : 0.000049\n","[epoch 81 / 100] average loss : 0.000049\n","[epoch 82 / 100] average loss : 0.000049\n","[epoch 83 / 100] average loss : 0.000049\n","[epoch 84 / 100] average loss : 0.000048\n","[epoch 85 / 100] average loss : 0.000048\n","[epoch 86 / 100] average loss : 0.000048\n","[epoch 87 / 100] average loss : 0.000047\n","[epoch 88 / 100] average loss : 0.000047\n","[epoch 89 / 100] average loss : 0.000047\n","[epoch 90 / 100] average loss : 0.000047\n","[epoch 91 / 100] average loss : 0.000046\n","[epoch 92 / 100] average loss : 0.000046\n","[epoch 93 / 100] average loss : 0.000046\n","[epoch 94 / 100] average loss : 0.000045\n","[epoch 95 / 100] average loss : 0.000045\n","[epoch 96 / 100] average loss : 0.000045\n","[epoch 97 / 100] average loss : 0.000045\n","[epoch 98 / 100] average loss : 0.000044\n","[epoch 99 / 100] average loss : 0.000044\n"]}],"source":["config = { 'learning_rate' : 0.1,\n","            'num_epoch' : 100\n","          }\n","\n","model = train_MLP(config)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":609,"status":"ok","timestamp":1680938238084,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"o5lDoJRK4fnT","outputId":"1af07e34-1543-40c2-ef2e-2768d9ad635b"},"outputs":[{"name":"stdout","output_type":"stream","text":["In train dataset ... \n","\t Accuracy : 0.7596333333333334\n","\n","In test dataset ... \n","\t Accuracy : 0.7677\n","\n"]}],"source":["def eval(model, train_version = True) :\n","    if train_version :\n","        x = train_img\n","        labels = train_label.squeeze()\n","        print('In train dataset ... ')\n","    else : \n","        x = test_img\n","        labels = test_label.squeeze()\n","        print('\\nIn test dataset ... ')\n","    \n","    for layer in model.values() :\n","        if isinstance(layer, softmax_with_crossEntropy) :\n","            x = layer.softmax_forward(x)\n","        else :\n","            x = layer.forward(x)\n","            \n","    preds = x.argmax(axis=1)\n","    acc = np.sum(np.where(preds==labels, True, False))/len(labels)\n","    return acc\n","\n","print('\\t Accuracy :', eval(model, train_version=True))\n","print('\\t Accuracy :', eval(model, train_version=False))\n","print()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Sigmoid와 Softmax는 학습 관점에서 크게 4가지 차이점이 있다."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["1. 출력값 : 두 함수 모두 출력값은 0과 1사이의 실수지만, Sigmoid는 입력값이 커질수록 1에 가까워 지고, 작아질수록 0에 가까워 진다. 반면 Softmax는 모든 클래스에 대한 확률에 합이 1이 되도록 정규화 된다."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["2. 사용 용도 : Sigmoid 함수는 주로 이진 분류 문제에 사용된다. 반면 Softmax 함수는 다중 클래스 분류 문제에 사용된다."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["3. 의존성 : Sigmoid 함수는 출력값이 독립적이므로 클래스 간 관계가 없지만, Softmax 함수는 모든 클래스에 대한 확률을 정규화하므로 관계가 있다."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["4. 다중 클래스 분류 : Sigmoid는 입력값에 대해 각각 클래스에 대한 확률을 독립적으로 계산하여, 클래스를 이진 분류를 수행한다. 반면, Softmax는 입력값에 대해 각 클래스에 대한 확률을 계산하고 클래스별 확률을 정규화하여 다중 클래스 분류를 수행한다."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aNUjBJXC4fnT"},"source":["## L2 Regularization"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### L2 Regularization을 통해 Overfitting을 방지하고 위의 학습한 데이터와 평가 및 차이를 알아보자"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1680938238084,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"XWigpHa54fnT"},"outputs":[],"source":["class L2_regularization :\n","    def __init__(self, lamb) :\n","        self.lamb = lamb\n","    \n","    def forward(self, W) :\n","        self.W = W\n","        self.regularization_term = 0.5 * self.lamb * np.sum(self.W**2)\n","        return self.regularization_term\n","\n","    def backward(self, W) :\n","        return self.lamb * W"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":568,"status":"ok","timestamp":1680939650640,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"cL53oKut4fnT"},"outputs":[],"source":["def train(config) :\n","    lr, num_epoch, weight_decay = config['learning_rate'], config['num_epoch'], config['weight_decay']\n","    print_loss_interval = 9\n","\n","    layer1_l2 = Linear(784, 100)\n","    relu_l2 = Relu()\n","    layer2_l2 = Linear(100, 10)\n","    softmax_with_CE_l2 = softmax_with_crossEntropy()\n","\n","    L2_reg = L2_regularization(weight_decay)  # L2 정규화 적용\n","    \n","    for iter in range(num_epoch) :\n","        # forward\n","        x = layer1_l2.forward(train_img)\n","        x = relu_l2.forward(x)\n","        x = layer2_l2.forward(x)\n","        preds = softmax_with_CE_l2.softmax_forward(x)\n","\n","        # loss\n","        one_hot_labels = make_one_hot(train_label)\n","        losses = softmax_with_CE_l2.crossEntropy_forward(preds, one_hot_labels)\n","        loss = losses.sum()/len(preds)\n","\n","        if iter % print_loss_interval == 0:\n","            print(\"[epoch %d / %d] average loss : %f\" % (iter, num_epoch, loss))\n","        \n","        # backward\n","        dL = softmax_with_CE_l2.backward()\n","        dL = layer2_l2.backward(dL, lr)\n","        dL = relu_l2.backward(dL)\n","        dL = layer1_l2.backward(dL, lr)\n","\n","        # weight update\n","        layer1_l2.W -= lr * L2_reg.backward(layer1_l2.W)\n","        layer2_l2.W -= lr * L2_reg.backward(layer2_l2.W)\n","\n","    model_l2 = OrderedDict()\n","    model_l2['layer1'] = layer1_l2\n","    model_l2['relu'] = relu_l2\n","    model_l2['layer2'] = layer2_l2\n","    model_l2['softmax_with_CE'] = softmax_with_CE_l2\n","    \n","    return model_l2"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":138347,"status":"ok","timestamp":1680946123661,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"lSRKwrH24fnT","outputId":"ced15f00-1767-4d3b-9d5a-e1c81abaa29e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[epoch 0 / 100] average loss : 0.000232\n","[epoch 9 / 100] average loss : 0.000126\n","[epoch 18 / 100] average loss : 0.000085\n","[epoch 27 / 100] average loss : 0.000065\n","[epoch 36 / 100] average loss : 0.000052\n","[epoch 45 / 100] average loss : 0.000042\n","[epoch 54 / 100] average loss : 0.000034\n","[epoch 63 / 100] average loss : 0.000027\n","[epoch 72 / 100] average loss : 0.000022\n","[epoch 81 / 100] average loss : 0.000018\n","[epoch 90 / 100] average loss : 0.000015\n","[epoch 99 / 100] average loss : 0.000013\n"]}],"source":["config = {  'learning_rate' : 0.1,\n","            'num_epoch' : 100,\n","            'weight_decay' : 0.1\n","          }\n","\n","model_l2 = train(config)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":578,"status":"ok","timestamp":1680946124234,"user":{"displayName":"이도형","userId":"06673665766385920340"},"user_tz":-540},"id":"XxZsbDAN4fnT","outputId":"7121b25a-7b62-471a-d196-e090814c774e"},"outputs":[{"name":"stdout","output_type":"stream","text":["In train dataset ... \n","\t Accuracy : 0.7596333333333334\n","\n","In test dataset ... \n","\t Accuracy : 0.7677\n","\n"," ---After L2 regularization---\n","In train dataset ... \n","\t Accuracy : 0.7961166666666667\n","\n","In test dataset ... \n","\t Accuracy : 0.7995\n","\n"]}],"source":["print('\\t Accuracy :', eval(model, train_version=True))\n","print('\\t Accuracy :', eval(model, train_version=False))\n","print('\\n ---After L2 regularization---')\n","print('\\t Accuracy :', eval(model_l2, train_version=True))\n","print('\\t Accuracy :', eval(model_l2, train_version=False))\n","print()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["L2 Regularization을 하였을 때는 하지 않았을 때에 비해 Accuracy가 높게 나온다"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Weight Decay에 따라 달라지는데 Weight Decay가 너무 크면 모델이 데이터에 충분히 적합하지 못하고, 너무 작으면 정규화 효과를 보지 못한다"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"ml","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"ac989702ee82cf85834fc904c407af640e06b0902e988e9cd8a067d885b3b790"}}},"nbformat":4,"nbformat_minor":0}
